{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "from sklearn.externals.six import StringIO  \n",
    "from IPython.display import Image  \n",
    "from sklearn.tree import export_graphviz\n",
    "import pydotplus\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import ensemble\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpd = pd.read_csv('../dataframe_creation/complete_dataframe.csv',index_col=0, parse_dates=True)\n",
    "fpd = fpd.fillna(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create predictor\n",
    "Create a function that can take the the data and create a proper predictor for each intersection and store them in a dataframe. Then, it will predict the outcomes for the test dataset and store the results in another dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictor(data,intersections,predictor,k):\n",
    "    predictors = {}\n",
    "    performance = {}\n",
    "    confusion_matrices = {}\n",
    "    features = {}\n",
    "    sel = ['K302','K173','K414','K158','K402','K305']\n",
    "    #print('K is set to ',k)\n",
    "    for label in intersections:\n",
    "        df = data.copy(deep=True)\n",
    "        print('\\nLabel: ',label)\n",
    "        y = df[label][2:] #set label to be current intersection\n",
    "        \n",
    "        boundary = y.sort_values().head(k)[-1]#Get K lowest LOF score and use as boundary \n",
    "        print('Boundary is set to: ',boundary)\n",
    "        y = y.apply(make_binary,boundary=boundary)\n",
    "        \n",
    "        y,y_test = split_data(y)\n",
    "        #print('Length of y: ',len(y))\n",
    "\n",
    "        for feature in intersections: # for each intersection create t-1 & t-2 feature\n",
    "            df['t-1'+feature] = df[feature].shift(periods=1)\n",
    "            df['t-2'+feature] = df[feature].shift(periods=2)\n",
    "            df = df.drop(feature,axis=1) #Then, drop the original column\n",
    "        \n",
    "        if label in sel:\n",
    "            X = df[2:]\n",
    "            X,X_test = split_data(X)\n",
    "            #print('Length of X: ',len(X))\n",
    "\n",
    "            features[label] = X.columns\n",
    "\n",
    "            #Create predictor and get best hyperparameters with grid search\n",
    "            if predictor == 'dtree':\n",
    "                model = dtree_grid_search(X,y)\n",
    "            if predictor == 'lr':\n",
    "                model = lr_grid_search(X,y)\n",
    "            if predictor == 'gb':\n",
    "                model = gb_grid_search(X,y)\n",
    "            if predictor == 'rf':\n",
    "                model = rf_grid_search(X,y)\n",
    "\n",
    "            #Test model performance and print results\n",
    "            y_pred = model.predict(X_test)\n",
    "            try:\n",
    "                auc = roc_auc_score(y_test, y_pred)\n",
    "                accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "                f1 = metrics.f1_score(y_test, y_pred)\n",
    "                precision = metrics.precision_score(y_test, y_pred)\n",
    "\n",
    "                print('\\nPrediction results: ')\n",
    "                print('Area under the ROC curve: ',auc)\n",
    "            except:\n",
    "                print('!!!Error here when runnig AUC!!!')\n",
    "            print(\"Accuracy:\",accuracy)\n",
    "            print(\"f1:\",f1)\n",
    "            print(\"Precision:\",precision)\n",
    "            print(\"Recall:\",metrics.recall_score(y_test, y_pred))\n",
    "            try:\n",
    "                tn,fp,fn,tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "                print('tn',tn, 'fp',fp, 'fn',fn, 'tp',tp)\n",
    "\n",
    "                predictors[label] = model\n",
    "                performance[label] = auc,accuracy,f1,precision\n",
    "                confusion_matrices[label] = list(confusion_matrix(y_test, y_pred).ravel())\n",
    "            except:\n",
    "                print('!!!Error in last part!!!')\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "    return predictors,performance,confusion_matrices,features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split data into train/val and test based on date\n",
    "def split_data(df):\n",
    "    df_test = df['2019-01-01':]\n",
    "    df = df[:'2018-12-31']\n",
    "    return df,df_test\n",
    "\n",
    "#Make the label a binary of outlier or not.\n",
    "def make_binary(row,boundary):\n",
    "    if row <= boundary:\n",
    "        row = 1\n",
    "    else:\n",
    "        row = 0\n",
    "    return row\n",
    "\n",
    "#Hyper parameter selection for Dtree\n",
    "def dtree_grid_search(X,y):\n",
    "    #param_grid = { 'criterion':['gini','entropy']}#,'min_samples_split' : [10]}#,'max_depth': [19]}\n",
    "    dtree_model=DecisionTreeClassifier()\n",
    "    #dtree_gscv = GridSearchCV(dtree_model, param_grid,cv=5, scoring=\"roc_auc\")\n",
    "    dtree_model.fit(X, y)\n",
    "    return dtree_model\n",
    "\n",
    "def lr_grid_search(X,y):\n",
    "    param_grid = {'C': [0.001, 0.01, 0.1, 1, 10], 'solver':['lbfgs'] }\n",
    "    lr = LogisticRegression()\n",
    "    model = GridSearchCV(lr, param_grid,cv=5,scoring=\"roc_auc\")\n",
    "    model.fit(X, y)\n",
    "    return model\n",
    "\n",
    "def gb_grid_search(X,y):\n",
    "    param_grid = {'learning_rate': [0.001, 0.01, 0.1, 1, 10] }\n",
    "    gb = ensemble.GradientBoostingClassifier()\n",
    "    model = GridSearchCV(gb, param_grid,cv=5, scoring=\"roc_auc\")\n",
    "    model.fit(X, y)\n",
    "    return model\n",
    "\n",
    "def rf_grid_search(X,y):\n",
    "    #param_grid = {'n_estimators': [10,100] }\n",
    "    rf = RandomForestClassifier(random_state = 2,n_estimators = 100)\n",
    "    #model = GridSearchCV(rf, param_grid,cv=5, scoring=\"roc_auc\")\n",
    "    rf.fit(X, y)\n",
    "    return rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scores(auc,f1,prec,title):\n",
    "    # set width of bar\n",
    "    barWidth = 0.25\n",
    "\n",
    "    # Set position of bar on X axis\n",
    "    r1 = np.arange(len(auc))\n",
    "    r2 = [x + barWidth for x in r1]\n",
    "    r3 = [x + barWidth for x in r2]\n",
    "\n",
    "    # Make the plot\n",
    "    plt.figure(figsize=(15,10))\n",
    "    plt.title(title)\n",
    "    plt.bar(r1, auc, color='#7f6d5f', width=barWidth, edgecolor='white', label='auc')\n",
    "    plt.bar(r2, f1, color='#557f2d', width=barWidth, edgecolor='white', label='f1')\n",
    "    plt.bar(r3, prec, color='#2d7f5e', width=barWidth, edgecolor='white', label='prec')\n",
    "\n",
    "    # Add xticks on the middle of the group bars\n",
    "    plt.xlabel('Location', fontweight='bold')\n",
    "    plt.xticks([r + barWidth for r in range(len(auc))], ints)\n",
    "\n",
    "    # Create legend & Show graphic\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing ground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intersections = [\n",
    "    'K302','K173','K414','K158','K402','K305',\n",
    "    'K071','K097','K124','K128','K159','K184','K189','K206','K225','K270','K304','K405','K406','K424','K430','K703','K704','K707','K711','K561','K504','K145','K250',\n",
    "    'a12in','a12out','n211in','n211out','n141in','n141out','n142in','n142out','n143in','n143out'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1,a2,a3,a4 = predictor(fpd,intersections,'rf',50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a110,a210,a310,a410 = predictor(fpd,intersections,'rf',10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a125,a225,a325,a425 = predictor(fpd,intersections,'rf',25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a150,a250,a350,a450 = predictor(fpd,intersections,'rf',50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a175,a275,a375,a475 = predictor(fpd,intersections,'rf',75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a1100,a2100,a3100,a4100 = predictor(fpd,intersections,'rf',100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a1150,a2150,a3150,a4150 = predictor(fpd,intersections,'rf',150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a1300,a2300,a3300,a4300 = predictor(fpd,intersections,'rf',300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a1500,a2500,a3500,a4500 = predictor(fpd,intersections,'rf',500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a11000,a21000,a31000,a41000 = predictor(fpd,intersections,'rf',1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a15000,a25000,a35000,a45000 = predictor(fpd,intersections,'rf',5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dec. Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b1,b2,b3,b4 = predictor(fpd,intersections,'dtree',50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1,c2,c3,c4 = predictor(fpd,intersections,'lr',50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize prediction results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bar group per intersection with AUC/F1/Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel = ['K302','K173','K414','K158','K402','K305']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = c2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ints = scores.keys()\n",
    "\n",
    "auc=[]\n",
    "acc=[]\n",
    "f1=[]\n",
    "prec=[]\n",
    "\n",
    "for i in ints:\n",
    "    if i in sel:\n",
    "        auc.append(scores[i][0])\n",
    "        acc.append(scores[i][1])\n",
    "        f1.append(scores[i][2])\n",
    "        prec.append(scores[i][3])\n",
    "    else: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scores(auc,f1,prec,'Random Forest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scores(auc,f1,prec,'Random Forest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dec. Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scores(auc,f1,prec,'Dec. Tree')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scores(auc,f1,prec,'Logistic regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare all intersections' ROC and see which ones are scoring least"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ints = a2.keys()\n",
    "\n",
    "auc=[]\n",
    "acc=[]\n",
    "f1=[]\n",
    "prec=[]\n",
    "\n",
    "for i in ints:\n",
    "    auc.append(a2[i][0])\n",
    "    acc.append(a2[i][1])\n",
    "    f1.append(a2[i][2])\n",
    "    prec.append(a2[i][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_scores(auc,f1,prec,'All Intersections')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(auc,title,ints):\n",
    "    y_pos = np.arange(len(auc))\n",
    "    # Make the plot\n",
    "    plt.figure(figsize=(15,10))\n",
    "    plt.title(title)\n",
    "    plt.barh(y_pos,auc[::-1], color='#7f6d5f', edgecolor='white', label='auc')\n",
    "\n",
    "    # Add xticks on the middle of the group baxrs\n",
    "    plt.ylabel('group', fontweight='bold')\n",
    "    plt.yticks([r for r in range(len(auc))], list(ints)[::-1])\n",
    "\n",
    "    # Create legend & Show graphic\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(auc,'All Intersections',ints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterate with increasing K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomes = [a210,a225,a250,a275,a2100,a2150,a2300,a2500,a21000,a25000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "li = []\n",
    "for s in sel:\n",
    "    auc = []\n",
    "    for outcome in outcomes:\n",
    "        scores = outcome\n",
    "        ints = scores.keys()\n",
    "        #auc=[]\n",
    "\n",
    "        for i in ints:\n",
    "            if i == s:\n",
    "                auc.append(scores[i][0])\n",
    "            else: pass\n",
    "    li.append(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticks = ['10','25','50','75','100','150','300','500','1000','5000']\n",
    "ndf = pd.DataFrame(li)\n",
    "ndf.columns = ticks\n",
    "ndf = ndf.swapaxes(\"index\", \"columns\")\n",
    "ndf.columns = sel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "plt.title('Random Forest AUC varying K')\n",
    "plt.plot(ndf)\n",
    "plt.xlabel('K', fontweight='bold')\n",
    "plt.xticks([r for r in range(len(auc))], ticks)\n",
    "plt.legend(ndf.columns)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "auc = li[2]\n",
    "barWidth = 0.25\n",
    "r1 = np.arange(len(auc))\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.title(sel[2])\n",
    "plt.bar(r1, auc, color='#7f6d5f', width=barWidth, edgecolor='white', label='auc')\n",
    "\n",
    "# Add xticks on the middle of the group bars\n",
    "plt.xlabel('K', fontweight='bold')\n",
    "plt.xticks([r for r in range(len(auc))], ticks)\n",
    "\n",
    "# Create legend & Show graphic\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
