{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "from sklearn.externals.six import StringIO  \n",
    "from IPython.display import Image  \n",
    "from sklearn.tree import export_graphviz\n",
    "import pydotplus\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import ensemble\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "import matplotlib.pyplot as plt\n",
    "from operator import itemgetter  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations = pd.read_csv('../calculated_correlation/correlation_current.csv')\n",
    "correlations = correlations.drop('Unnamed: 0',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpd = pd.read_csv('../dataframe_creation/complete_dataframe.csv',index_col=0, parse_dates=True)\n",
    "fpd = fpd.fillna(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel = ['K302','K173','K414','K158','K402','K305']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scores(auc,f1,prec,title):\n",
    "    # set width of bar\n",
    "    barWidth = 0.25\n",
    "\n",
    "    # Set position of bar on X axis\n",
    "    r1 = np.arange(len(auc))\n",
    "    r2 = [x + barWidth for x in r1]\n",
    "    r3 = [x + barWidth for x in r2]\n",
    "\n",
    "    # Make the plot\n",
    "    plt.figure(figsize=(15,10))\n",
    "    plt.title(title)\n",
    "    plt.bar(r1, auc, color='#7f6d5f', width=barWidth, edgecolor='white', label='auc')\n",
    "    plt.bar(r2, f1, color='#557f2d', width=barWidth, edgecolor='white', label='f1')\n",
    "    plt.bar(r3, prec, color='#2d7f5e', width=barWidth, edgecolor='white', label='prec')\n",
    "\n",
    "    # Add xticks on the middle of the group bars\n",
    "    plt.xlabel('Location', fontweight='bold')\n",
    "    plt.xticks([r + barWidth for r in range(len(auc))], ints)\n",
    "    # Create legend & Show graphic\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictor(data,intersections,predictor,k,features):\n",
    "    predictors = {}\n",
    "    performance = {}\n",
    "    confusion_matrices = {}\n",
    "    used_features = {}\n",
    "    importances = {}\n",
    "    sel = ['K302','K173','K414','K158','K402','K305']\n",
    "    \n",
    "    for label in intersections:\n",
    "        try:\n",
    "            feats = features[label]\n",
    "        except:\n",
    "            return [predictors,performance,confusion_matrices,used_features,importances]\n",
    "        print('\\nLabel: ',label)\n",
    "        df = data.copy(deep=True)\n",
    "        df = df[feats]\n",
    "        y = df[label][2:] #set label to be current intersection\n",
    "        \n",
    "        boundary = y.sort_values().head(k)[-1]#Get K lowest LOF score and use as boundary \n",
    "        print('Boundary is set to: ',boundary)\n",
    "        y = y.apply(make_binary,boundary=boundary)\n",
    "        \n",
    "        y,y_test = split_data(y)\n",
    "        #print('Length of y: ',len(y))\n",
    "\n",
    "        for feature in intersections: # for each intersection create t-1 & t-2 feature\n",
    "            if feature in list(df.columns):\n",
    "                df['t-1'+feature] = df[feature].shift(periods=1)\n",
    "                df['t-2'+feature] = df[feature].shift(periods=2)\n",
    "                #df['t-3'+feature] = df[feature].shift(periods=3)\n",
    "                #df['t-4'+feature] = df[feature].shift(periods=4)\n",
    "                df = df.drop(feature,axis=1) #Then, drop the original column\n",
    "            else:\n",
    "                pass\n",
    "        \n",
    "        if label in sel:\n",
    "            X = df[2:]\n",
    "            X,X_test = split_data(X)\n",
    "            #print('Length of X: ',len(X))\n",
    "\n",
    "            used_features[label] = X.columns\n",
    "\n",
    "            #Create predictor and get best hyperparameters with grid search\n",
    "            if predictor == 'rf':\n",
    "                model = rf_grid_search(X,y)\n",
    "\n",
    "            \n",
    "            #Test model performance and print results\n",
    "            y_pred = model.predict(X_test)\n",
    "            try:\n",
    "                auc = roc_auc_score(y_test, y_pred)\n",
    "                accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "                f1 = metrics.f1_score(y_test, y_pred)\n",
    "                precision = metrics.precision_score(y_test, y_pred)\n",
    "\n",
    "                print('\\nPrediction results: ')\n",
    "                print('Area under the ROC curve: ',auc)\n",
    "            except:\n",
    "                print('!!!Error here when runnig AUC!!!')\n",
    "            print(\"Accuracy:\",accuracy)\n",
    "            print(\"f1:\",f1)\n",
    "            print(\"Precision:\",precision)\n",
    "            print(\"Recall:\",metrics.recall_score(y_test, y_pred))\n",
    "            try:\n",
    "                tn,fp,fn,tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "                print('tn',tn, 'fp',fp, 'fn',fn, 'tp',tp)\n",
    "\n",
    "                predictors[label] = model\n",
    "                performance[label] = auc,accuracy,f1,precision\n",
    "                confusion_matrices[label] = list(confusion_matrix(y_test, y_pred).ravel())\n",
    "                importances[label] = model.feature_importances_\n",
    "                \n",
    "                #if label == 'K414':\n",
    "                    \n",
    "                \n",
    "            except:\n",
    "                print('!!!Error in last part!!!')\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "    return [predictors,performance,confusion_matrices,used_features,importances]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split data into train/val and test based on date\n",
    "def split_data(df):\n",
    "    df_test = df['2019-01-01':]\n",
    "    df = df[:'2018-12-31']\n",
    "    return df,df_test\n",
    "\n",
    "#Make the label a binary of outlier or not.\n",
    "def make_binary(row,boundary):\n",
    "    if row <= boundary:\n",
    "        row = 1\n",
    "    else:\n",
    "        row = 0\n",
    "    return row\n",
    "\n",
    "def rf_grid_search(X,y):\n",
    "    #param_grid = {'n_estimators': [10,100] }\n",
    "    rf = RandomForestClassifier(random_state = 1,n_estimators = 100)\n",
    "    #model = GridSearchCV(rf, param_grid,cv=5, scoring=\"roc_auc\")\n",
    "    rf.fit(X, y)\n",
    "    return rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intersections = [\n",
    "    'K302','K173','K414','K158','K402','K305',\n",
    "    'K071','K097','K124','K128','K159','K184','K189','K206','K225','K270','K304','K405','K406','K424','K430','K703','K704','K707','K711','K561','K504','K145','K250',\n",
    "    'a12in','a12out','n211in','n211out','n141in','n141out','n142in','n142out','n143in','n143out'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection(treshold,cor_data):\n",
    "    features = {}\n",
    "    for i in range(6):\n",
    "        feature_selection = []\n",
    "        for (columnName, columnData) in cor_data.loc[i].iteritems():\n",
    "            if float(columnData) > treshold:\n",
    "                feature_selection.append(columnName)\n",
    "            else:\n",
    "                pass  \n",
    "        features[sel[i]] = feature_selection\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#features = feature_selection(0.4,correlations)\n",
    "#a14,a24,a34,a44,feat4 = predictor(fpd,intersections,'rf',50,features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#features = feature_selection(0.3,correlations)\n",
    "#a13,a23,a33,a43,feat3 = predictor(fpd,intersections,'rf',50,features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#features = feature_selection(0.2,correlations)\n",
    "#a12,a22,a32,a42,feat2 = predictor(fpd,intersections,'rf',50,features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#features = feature_selection(0.1,correlations)\n",
    "#a11,a21,a31,a41,feat1 = predictor(fpd,intersections,'rf',50,features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = feature_selection(0.05,correlations)\n",
    "a50 = predictor(fpd,intersections,'rf',50,features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#features = feature_selection(0.0,correlations)\n",
    "#a10,a20,a30,a40,feat0= predictor(fpd,intersections,'rf',50,features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=a205\n",
    "y=a22\n",
    "ints = x.keys()\n",
    "\n",
    "auc=[]\n",
    "acc=[]\n",
    "f1=[]\n",
    "prec=[]\n",
    "\n",
    "for i in ints:\n",
    "    if i in sel:\n",
    "        '''\n",
    "        if i =='K302':\n",
    "            auc.append(y[i][0])\n",
    "            acc.append(y[i][1])\n",
    "            f1.append(y[i][2])\n",
    "            prec.append(y[i][3])            \n",
    "        else:'''\n",
    "        auc.append(x[i][0])\n",
    "        acc.append(x[i][1])\n",
    "        f1.append(x[i][2])\n",
    "        prec.append(x[i][3])\n",
    "    else: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scores(auc,f1,prec,'Random Forest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for intersection in sel:\n",
    "    print('\\n',intersection)\n",
    "    columns = a42[intersection]\n",
    "    importance = feat2[intersection]\n",
    "\n",
    "    feature_importance = dict(zip(columns, importance))\n",
    "    for key, value in sorted(feature_importance.items(), key = itemgetter(1), reverse = True):\n",
    "        print(key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a43['K302']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat3['K302']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlabs = ['0','0.05','0.1','0.2','0.3','0.4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cors(xlabs,sel):\n",
    "    for i in sel:\n",
    "        inters = i\n",
    "        z = []\n",
    "        z.append(a20[inters][0])\n",
    "        z.append(a205[inters][0])\n",
    "        z.append(a21[inters][0])\n",
    "        z.append(a22[inters][0])\n",
    "        z.append(a23[inters][0])\n",
    "        z.append(a24[inters][0])\n",
    "\n",
    "        plt.figure(figsize=(15,10))\n",
    "        plt.title(i)\n",
    "        plt.bar(np.arange(len(z)),z, color='#7f6d5f', edgecolor='white', label='auc')\n",
    "        plt.xlabel('group', fontweight='bold')\n",
    "        plt.xticks([r for r in range(len(z))], xlabs)\n",
    "        plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cors(xlabs,sel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize dec. tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intkey = 'K414'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = a105[intkey].estimators_[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_names = a405[intkey]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['0','1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export as dot file\n",
    "export_graphviz(estimator, out_file='tree.dot', \n",
    "                feature_names = feat_names,\n",
    "                class_names = labels,\n",
    "                rounded = True, proportion = False, \n",
    "                precision = 2, filled = True)\n",
    "\n",
    "# Convert to png using system command (requires Graphviz)\n",
    "from subprocess import call\n",
    "call(['dot', '-Tpng', 'tree.dot', '-o', 'tree.png', '-Gdpi=600'])\n",
    "\n",
    "# Display in jupyter notebook\n",
    "from IPython.display import Image\n",
    "Image(filename = 'tree.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
