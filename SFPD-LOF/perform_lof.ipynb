{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform Local Outlier Factor\n",
    "Gather the created FPD streams and perform the Local Outlier Factor method to detect outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "import datetime as dt\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load FPDs\n",
    "Load the FPDs json file to use in the program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fpds(file):\n",
    "    with open(file) as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize FPDs\n",
    "Vectorize each FPD with the length of the vector as the maximum flow measurement. The goal is to create a range from zero to max and attach the probabilities accordingly. If the value is not in the FPD, the probability is zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_fpd(fpd):\n",
    "    vector_list = list(range(max(fpd[0])+1))\n",
    "    vector = []\n",
    "    for value in vector_list:\n",
    "        if value in(fpd[0]):\n",
    "            vector.append(fpd[1][fpd[0].index(value)])\n",
    "        else:\n",
    "            vector.append(0)\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector sizing\n",
    "If two vectors are of a different size, then make the shortest vector longer so that both are the same size. Do this by adding zero values to the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_sizing(vector1,vector2):\n",
    "    size1,size2 = len(vector1),len(vector2)\n",
    "    if size1>size2:\n",
    "        added_values = list(range(size2,size1))\n",
    "        vector2.extend([0 for added_value in added_values])\n",
    "        return vector1,vector2\n",
    "    if size2>size1:\n",
    "        added_values = list(range(size1,size2))\n",
    "        vector1.extend([0 for added_value in added_values])\n",
    "        return vector1,vector2\n",
    "    else:\n",
    "        return vector1,vector2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bhattacharyya distance for vectors\n",
    "Function to compute the distance between two fpd vectors. The measure used for this is the Bhattacharyya distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bhatta(vector1,vector2):\n",
    "    if vector1 == vector2: #redundant as of new sim measure?\n",
    "        return 0.0\n",
    "    else:\n",
    "        try: #This should no longer be needed when we clean the 0 values\n",
    "            vector1,vector2 = vector_sizing(vector1,vector2)\n",
    "            v1 = np.average(vector1)\n",
    "            v2 = np.average(vector2)\n",
    "            distance = 0\n",
    "            for i in range(len(vector1)):\n",
    "                distance += math.sqrt( vector1[i] * vector2[i] )\n",
    "            distance = math.sqrt( 1 - ( 1 / math.sqrt(v1*v2*len(vector1)*len(vector2)) ) * distance)\n",
    "            return distance\n",
    "        except:\n",
    "            return 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity measure\n",
    "Function that uses the previous functions and combines them to be able to calculate the similarity of 1 fpd to all other fpds. Do an all to all calculation of the distances without doing redundant calculations. Use the bhatta function in this function to calculate distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_measure(stream):\n",
    "    distances_matrix = []\n",
    "    to_skip = []\n",
    "    for fpd_one in stream: #for each fpd in stream\n",
    "        distances_vector = [] #create a vector to be filled with the distances\n",
    "        fpd_vector1 = vectorize_fpd(stream[fpd_one])\n",
    "        for fpd_two in stream:\n",
    "            fpd_vector2 = vectorize_fpd(stream[fpd_two]) #vectorize the item from which to be measured\n",
    "            to_skip.append([fpd_one,fpd_two])\n",
    "            distances_vector.append([bhatta(fpd_vector1,fpd_vector2)]) #measure distance and append\n",
    "        distances_matrix.append(distances_vector)\n",
    "    return [[value for [value] in vector]for vector in distances_matrix]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Outlier Factor\n",
    "\n",
    "First get the k closest fpds\n",
    "\n",
    "Second calculate LRD\n",
    "\n",
    "Third calculate LOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_KNN(distances_stream,k):\n",
    "    #Get the k smallest distances in one vector by sorting and then slicing\n",
    "    #Copy to be sure there are no errors due to sorting\n",
    "    copy_stream = distances_stream.copy()\n",
    "    #Add index so that we are able to find the original index before sorting\n",
    "    indexes = list(range(len(copy_stream)))\n",
    "    indexed_copy_stream = list(zip(indexes,copy_stream))\n",
    "    indexed_copy_stream = sorted(indexed_copy_stream, key = lambda x: x[1]) # sort based on second value\n",
    "    return indexed_copy_stream[1:k+1] #Skip the first one, since that is always self which is 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_reachability_density(f,distances_matrix,k):\n",
    "    #Sum up all the distances in knn and divide it by k, which is the length of knn and divide by 1\n",
    "    knn = get_KNN(distances_matrix[f],k) #KNN from the vector of the matrix of f\n",
    "    knn_distances = [a[1] for a in knn]\n",
    "    if sum(knn_distances) == 0:\n",
    "        return 0\n",
    "    return 1/(sum(knn_distances)/len(knn_distances))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_outlier_factor(f,distances_matrix,k):\n",
    "    #Calculate the lrd of f and calculate the lrd of the knn's then devide these and sum it up.\n",
    "    lof = 0\n",
    "    knn = get_KNN(distances_matrix[f],k)\n",
    "    neighbours = [a[0] for a in knn] #list of indexes of the neighbours\n",
    "    lrd_one = local_reachability_density(f,distances_matrix,k)\n",
    "    if lrd_one == 0:\n",
    "        return 0\n",
    "    for neighbour in neighbours: #calculate the lrd for each neighbour\n",
    "        lrd_two = local_reachability_density(neighbour,distances_matrix,k)\n",
    "        if lrd_two == 0:\n",
    "            return 0\n",
    "        fraction = lrd_two/lrd_one\n",
    "        lof = lof+fraction\n",
    "    if lof == 0:\n",
    "        return 0\n",
    "    return 1/(lof/k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_lof(stream,k,hour):\n",
    "    temp = {sum(value[0])/len(value[0]) for (key,value) in stream.items()}\n",
    "    stream_average = sum(temp)/len(temp)\n",
    "    dates = list(stream.keys())\n",
    "    start2 = time.time()\n",
    "    #Calculate the lof for all fpds in stream and return a list of lof scores with  \n",
    "    #matching index (could become dates if necessary). \n",
    "    lof_scores = []\n",
    "    #Create distances matrix\n",
    "    distances = similarity_measure(stream)\n",
    "    \n",
    "    for fpd in range(len(distances[0])): #For each fpd\n",
    "        average = sum(stream[list(stream.keys())[fpd]][0])/len(stream[list(stream.keys())[fpd]][0])\n",
    "        if average > stream_average:\n",
    "            high_traffic = 1\n",
    "        else:\n",
    "            high_traffic = 0\n",
    "        lof_scores.append([dates[fpd],hour,local_outlier_factor(fpd,distances,k),high_traffic]) #Calculate lof_scores\n",
    "    end2 = time.time()\n",
    "    print('Time taken for this stream: ',end2 - start2)\n",
    "    return lof_scores #Return list of scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating outlier lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_outliers(file,k):\n",
    "    start1 = time.time()\n",
    "    fpd_file = load_fpds(file)\n",
    "    intersections = list(fpd_file.keys())\n",
    "    #set up weekdays list containing weekdays that are present in the dict\n",
    "    weekdays = list(fpd_file[intersections[0]].keys())\n",
    "    #set up hours list containing hours that are present in the dict\n",
    "    hours = list(fpd_file[intersections[0]][weekdays[0]].keys())\n",
    "    #Set up the dictionary to be filled with outliers\n",
    "    outliers = create_outlier_dict(intersections)\n",
    "    \n",
    "    #We feed a single stream to the similarity measure function.\n",
    "    #Therefore, we should go through the dict and pick up single streams and then feed them.\n",
    "    for intersection in intersections:\n",
    "        for weekday in weekdays:\n",
    "            for hour in hours:\n",
    "                print('Starting LOF calculation for stream: ',intersection,weekday,hour)\n",
    "                outliers[intersection][weekday][hour] = list(perform_lof(fpd_file[intersection][weekday][hour],k,hour))\n",
    "        with open(str(intersection)+'.json', 'w') as f:\n",
    "            json.dump(outliers, f)\n",
    "    end1 = time.time()\n",
    "    print('LOF algorithm is finished! It took ',end1-start1,' to perform the calculations in total.')\n",
    "    return outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_outlier_dict(intersections):\n",
    "    fpds = {}\n",
    "    hours = [(dt.time(i).strftime('%H')) for i in range(24)] #Create list of hours\n",
    "    for i in range(len(intersections)): #each intersection\n",
    "        fpds[intersections[i]] = {}\n",
    "        for weekday in range(7): #Set up day of the week\n",
    "            weekday = str(weekday)\n",
    "            fpds[intersections[i]][weekday] = {}\n",
    "    return fpds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'detect_outliers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-442f3f048882>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moutliers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetect_outliers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./edited_data/fpd_collection.json'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'detect_outliers' is not defined"
     ]
    }
   ],
   "source": [
    "outliers = detect_outliers('./edited_data/fpd_collection.json',k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./edited_data/outliers_collection.json', 'w') as f:\n",
    "    json.dump(outliers, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
